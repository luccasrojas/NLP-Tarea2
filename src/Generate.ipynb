{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 3 Recuperación ranqueada y vectorización de documentos (RRDV)\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de la perplejidad de cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "PATH_NEWS_UNIGRAM=\"../data/20N_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "PATH_BAC_UNIGRAM=\"../data/BAC_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "PATH_NEWS_BIGRAM=\"../data/20N_l.rojasb_j.arboleda_bigrams.txt\"\n",
    "PATH_BAC_BIGRAM=\"../data/BAC_l.rojasb_j.arboleda_bigrams.txt\"\n",
    "PATH_NEWS_TRIGRAM=\"../data/20N_l.rojasb_j.arboleda_trigrams.txt\"\n",
    "PATH_BAC_TRIGRAM=\"../data/BAC_l.rojasb_j.arboleda_trigrams.txt\"\n",
    "\n",
    "PATH_BAC_TEST = \"../data/BAC_l.rojasb_j.arboleda_test.txt\" \n",
    "PATH_NEWS_TEST = \"../data/20N_l.rojasb_j.arboleda_test.txt\"  \n",
    "\n",
    "tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(path):\n",
    "    \"\"\"\n",
    "    Load test data from a file.\n",
    "    Params\n",
    "    ------\n",
    "    path: str\n",
    "        Path to the file.\n",
    "    Returns\n",
    "    -------\n",
    "    test: list\n",
    "        List of strings.\n",
    "    \"\"\"\n",
    "    test = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            test.append(line.strip())\n",
    "    return test\n",
    "bac_test = load_test(PATH_BAC_TEST)\n",
    "news_test = load_test(PATH_NEWS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path:str)->dict:\n",
    "    #{\"<bigram>\":{\"count\":0, \"probability\":0}}\n",
    "    \"\"\"\n",
    "    Load a model from a file.\n",
    "    Params\n",
    "    ------\n",
    "    path: str\n",
    "        Path to the file.\n",
    "    Returns\n",
    "    -------\n",
    "    model: dict\n",
    "        Dictionary with the model.\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            model[line[0]] = int(line[1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk(sentence:str,monogram:dict)->str:\n",
    "    \"\"\"\n",
    "    Replace unknown words with <UNK>.\n",
    "    Params\n",
    "    ------\n",
    "    sentence: str\n",
    "        Sentence to be replaced.\n",
    "    Returns\n",
    "    -------\n",
    "    sentence: str\n",
    "        Sentence with <unk>.\n",
    "    \"\"\"\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    for word in words:\n",
    "        if word not in monogram:\n",
    "            sentence = sentence.replace(word, \"<UNK>\")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_perplexity(model, test_data)->float:\n",
    "    \"\"\"    \n",
    "    Calculate the perplexity of a unigram model.\n",
    "    Params\n",
    "    ------\n",
    "    model: dict\n",
    "        Dictionary with the model.\n",
    "    test_data: list\n",
    "        List of strings.\n",
    "    Returns\n",
    "    -------\n",
    "    perplexity: float\n",
    "        Perplexity of the model.\n",
    "    \"\"\"\n",
    "    test_size = len(test_data)\n",
    "    log_sum = 0\n",
    "    for sentence in test_data:\n",
    "        sentence = replace_unk(sentence, model)\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word in model:\n",
    "                probability = model[word][\"probability\"]\n",
    "            else:\n",
    "                probability = model[\"<UNK>\"]\n",
    "            log_sum += math.log2(probability)\n",
    "    return -1*(log_sum / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_perplexity(bigram_model, monogram_model, test_data)->float:\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a bigram model.\n",
    "    Params\n",
    "    ------\n",
    "    model: dict\n",
    "        Dictionary with the model.\n",
    "    test_data: list\n",
    "        List of strings.\n",
    "    Returns\n",
    "    -------\n",
    "    perplexity: float  \n",
    "        Perplexity of the model.\n",
    "    \"\"\"\n",
    "    test_size = len(test_data)\n",
    "    log_sum = 0\n",
    "    vocabulary = len(monogram_model)\n",
    "    for sentence in test_data:\n",
    "        sentence = replace_unk(sentence, monogram_model)\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for i in range(len(words)-1):\n",
    "            bi_gram = words[i]+\" \"+words[i+1]\n",
    "            if bi_gram in bigram_model:\n",
    "                probability = bigram_model[bi_gram][\"probability\"]\n",
    "            else:\n",
    "                probability = 1/monogram_model[words[i]][\"count\"]+vocabulary\n",
    "\n",
    "            log_sum += math.log2(probability)\n",
    "    return -1*(log_sum / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trigram_perplexity(trigram, bigram_model, monogram_model, test_data)->float:\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a trigram model.\n",
    "    Params\n",
    "    ------\n",
    "    model: dict\n",
    "        Dictionary with the model.\n",
    "    test_data: list\n",
    "        List of strings.\n",
    "    Returns\n",
    "    -------\n",
    "    perplexity: float  \n",
    "        Perplexity of the model.\n",
    "    \"\"\"\n",
    "    test_size = len(test_data)\n",
    "    log_sum = 0\n",
    "    vocabulary = len(monogram_model)\n",
    "    for sentence in test_data:\n",
    "        sentence = replace_unk(sentence, monogram_model)\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for i in range(len(words)-2):\n",
    "            tri_gram = words[i]+\" \"+words[i+1]+\" \"+words[i+2]\n",
    "            bi_gram = words[i]+\" \"+words[i+1]\n",
    "            if tri_gram in trigram:\n",
    "                probability = trigram[tri_gram][\"probability\"]\n",
    "            elif bi_gram in bigram_model:\n",
    "                probability = 1/(bigram_model[bi_gram][\"count\"]+vocabulary^2)\n",
    "            else:\n",
    "                probability = 1/(1+vocabulary^2)\n",
    "            log_sum += math.log2(probability)\n",
    "    return -1*(log_sum / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trigram2predict(trigram_model:dict):\n",
    "    \"\"\"\n",
    "    Convert a trigram model to a bigram model that predicts the next word.\n",
    "    Params\n",
    "    ------\n",
    "    trigram_model: dict\n",
    "        Dictionary with the trigram model.\n",
    "    Returns\n",
    "    -------\n",
    "    trigram_model: dict\n",
    "        Dictionary with bigrams and probabilities of other trigrams.\n",
    "    \"\"\"\n",
    "    bigrams = {}\n",
    "    for trigram in trigram_model:\n",
    "        words = tokenizer.tokenize(trigram)\n",
    "        bi_gram = words[0]+\" \"+words[1]\n",
    "        if bi_gram not in bigrams:\n",
    "            bigrams[bi_gram] = {\"prediction\":[words[2]], \"probability\":[trigram_model[trigram][\"probability\"]]}\n",
    "        else:\n",
    "            bigrams[bi_gram][\"prediction\"].append(words[2])\n",
    "            bigrams[bi_gram][\"probability\"].append(trigram_model[trigram][\"probability\"])\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bigram2predict(bigram_model:dict):\n",
    "    \"\"\"\n",
    "    Convert a bigram model to a monogram model that predicts the next word.\n",
    "    Params\n",
    "    ------\n",
    "    trigram_model: dict\n",
    "        Dictionary with the bigram model.\n",
    "    Returns\n",
    "    -------\n",
    "    trigram_model: dict\n",
    "        Dictionary with monograms and probabilities of other bigrams.\n",
    "    \"\"\"\n",
    "    monograms = {}\n",
    "    for bigram in bigram_model:\n",
    "        words = tokenizer.tokenize(bigram)\n",
    "        monogram = words[0]\n",
    "        if monogram not in monograms:\n",
    "            monograms[monogram] = {\"prediction\":[words[1]], \"probability\":[bigram_model[bigram][\"probability\"]]}\n",
    "        else:\n",
    "            monograms[monogram][\"prediction\"].append(words[1])\n",
    "            monograms[monogram][\"probability\"].append(bigram_model[bigram][\"probability\"])\n",
    "    return monograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_sentences(first_word:str, model:dict)->str:\n",
    "    \"\"\"\n",
    "    Generate a sentence from a model.\n",
    "    Params\n",
    "    ------\n",
    "    model: dict\n",
    "        Dictionary with the model.\n",
    "    Returns\n",
    "    -------\n",
    "    sentence: str\n",
    "        Generated sentence.\n",
    "    \"\"\"\n",
    "    monogram = False\n",
    "    bigram = False\n",
    "    trigram = False\n",
    "    if list(model.keys())[0].count(\" \") == 1:\n",
    "        model = convert_bigram2predict(model)\n",
    "        bigram = True\n",
    "    elif list(model.keys())[0].count(\" \") == 2:\n",
    "        model = convert_trigram2predict(model)\n",
    "        trigram = True\n",
    "    else:\n",
    "        monogram = True\n",
    "\n",
    "    sentence = \"<s> \"+first_word\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    while words[-1] != \"</s>\":\n",
    "        actual_word= words[-1]\n",
    "        prev_word = words[-2]\n",
    "        if bigram:\n",
    "            pre_predict = \"\"\n",
    "        elif trigram:\n",
    "            pre_predict = prev_word+\" \"+actual_word\n",
    "        if monogram:\n",
    "            predicted_word = random.choices(list(model.keys()), weights=list(model.values()))\n",
    "        elif bigram:\n",
    "            predicted_word = random.choices(model[pre_predict][\"prediction\"], weights=model[pre_predict][\"probability\"])\n",
    "        elif trigram:\n",
    "            predicted_word = random.choices(model[pre_predict][\"prediction\"], weights=model[pre_predict][\"probability\"])\n",
    "        sentence += \" \"+predicted_word\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "    return sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
