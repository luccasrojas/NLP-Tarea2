{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construccion del modelo de lenguaje\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo es la union de todos los documentos en un solo un par de archivos, uno con los documentos de 20news y otro con lso documentos de BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continucion se recorren todos los documentos de ambas carpetas y se unen en un solo archivo.\n",
    "* Se debe modificar la ruta de los archivos para que se ajuste. La ruta de la carpeta de 20news debe estar en PATH_20NEWS y la ruta de la carpeta de BAC debe estar en PATH_BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_20NEWS = '../data/20news-18828'\n",
    "PATH_FINAL_20NEWS = \"../data/final_20news.txt\"\n",
    "\n",
    "def load_news(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "\n",
    "    \"\"\"\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\") as final_document:\n",
    "        for folder in os.listdir(documents_path):\n",
    "            for document_file in os.listdir(os.path.join(documents_path, folder)):\n",
    "                with open(os.path.join(documents_path, folder, document_file), \"r\") as document:\n",
    "                    text = document.read()\n",
    "                final_document.write(text)\n",
    "                final_document.write(\"\\n\")\n",
    "\n",
    "load_news(PATH_20NEWS,PATH_FINAL_20NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PATH_BAC= '../data/BAC/blogs/blogs'\n",
    "PATH_FINAL_BAC = \"../data/final_bac.txt\"\n",
    "\n",
    "def load_bac(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "    \"\"\"\n",
    "    pattern = r'<post>(.*?)</post>'\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\", encoding = 'latin_1') as final_document:\n",
    "        for file_name in os.listdir(documents_path):\n",
    "            with open(os.path.join(PATH_BAC,file_name) , encoding=\"latin_1\") as f:\n",
    "                text = f.read()\n",
    "                texts = re.findall(pattern, text, re.DOTALL)\n",
    "            all_text= \". \\n\".join(texts)\n",
    "            filtered_text = all_text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "            final_document.write(filtered_text)\n",
    "\n",
    "\n",
    "load_bac(PATH_BAC,PATH_FINAL_BAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_FINAL_20NEWS, \"r\") as f:\n",
    "    raw_news = f.read()\n",
    "with open(PATH_FINAL_20NEWS, \"r\") as f:\n",
    "    raw_bac = f.read()\n",
    "\n",
    "news_sentences = sent_tokenize(raw_news)\n",
    "bac_sentences = sent_tokenize(raw_bac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente se leen los archivos y se normalizan con el formato adecuado para el modelo de lenguaje. De este modo se pone todo el minusculas, se remplazan los numeros por num y se agregan caracteres al inicio y al final de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence:str)->str:\n",
    "    \"\"\"\n",
    "    Normalize a sentence by lowercasing it, replacing numbers with NUM and adding <s> and </s> tokens\n",
    "    Params:\n",
    "    -------\n",
    "        sentence: sentence to normalize\n",
    "    Returns:\n",
    "    --------\n",
    "        sentence: normalized sentence\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower().replace (\"\\n\", \" \")\n",
    "    words = word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        try:\n",
    "            word.replace(\",\",\"\").replace(\".\",\"\").replace(\"-\",\"\").replace(\"$\",\"\").replace(\"'\",\"\")\n",
    "            number = float(word)\n",
    "            sentence = sentence.replace(word, \"NUM\")\n",
    "        except:\n",
    "            pass\n",
    "    sentence = f\"<s> {sentence} </s>\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego se extrae el vocabulario de todas las palabras junto con su frecuencia para asi poder reemplazar los tokens que no se encuentren en el vocabulario por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(sentences:list)->dict:\n",
    "    \"\"\"\n",
    "    Extract the vocabulary from a list of sentences\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "    Returns:\n",
    "    --------\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    \"\"\"\n",
    "    num_sentences = len(sentences)\n",
    "    vocabulary = {\"<s>\": num_sentences, \"</s>\": num_sentences}\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = 1\n",
    "            else:   \n",
    "                vocabulary[word] += 1\n",
    "    not_vocabulary = [\">\", \"<\", \"s\", \"/\",\":\",\"@\",\"..\"]\n",
    "    for word in not_vocabulary:\n",
    "        if word in vocabulary:\n",
    "            del vocabulary[word]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con el vocabulario y las frases se cambian todos los elementos que aparecen sola una vez en el corpus por el token \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknowns(sentences:list, vocabulary:dict)->list:\n",
    "    \"\"\"\n",
    "    Replace the words that appear only once in the corpus by the <UNK> token\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    Returns:\n",
    "    --------\n",
    "        sentences: list of sentences with the <UNK> token\n",
    "    \"\"\"\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = word_tokenize(sentence)\n",
    "        for j, word in enumerate(words):\n",
    "            try:\n",
    "                if vocabulary[word] == 1:\n",
    "                    sentences[i] = sentence.replace(word, \"<UNK>\")\n",
    "            except:\n",
    "                continue\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este punto se normalizan los 2 sets y se extrae su vocabulario para si poder remplazar los tokens que se encuentren en el vocabulario solo una vez por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_news_sentences = [normalize(sentence) for sentence in news_sentences]\n",
    "normalized_bac_sentences = [normalize(sentence) for sentence in bac_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vocabulary = extract_vocabulary(normalized_news_sentences)\n",
    "bac_vocabulary = extract_vocabulary(normalized_bac_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sentences = replace_unknowns(normalized_news_sentences, news_vocabulary)\n",
    "bac_sentences = replace_unknowns(normalized_bac_sentences, bac_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(normalized_news_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "news_train, news_test = train_test_split(news_sentences, test_size=0.20)\n",
    "bac_train, bac_test = train_test_split(bac_sentences, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego guardamos estos datos en 4 archivos, 2 para train y 2 para test, uno para 20news y otro para BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BAC_TRAIN = \"../data/20N_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_BAC_TEST = \"../data/20N_l.rojasb_j.arboleda_test.txt\"  \n",
    "PATH_NEWS_TRAIN = \"../data/BAC_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_NEWS_TEST = \"../data/BAC_l.rojasb_j.arboleda_test.txt\" \n",
    "\n",
    "def save_file(sentences:list, path:str):\n",
    "    \"\"\"\n",
    "    Save a list of sentences in a file\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        path: path to the file\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "save_file(news_train, PATH_NEWS_TRAIN)\n",
    "save_file(news_test, PATH_NEWS_TEST)\n",
    "save_file(bac_train, PATH_BAC_TRAIN)\n",
    "save_file(bac_test, PATH_BAC_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego pasamos a crear los n-gramas, para esto construimos diccionarios para cada combinacion posible de n-gramas, en este caso se construyen diccionario para los monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_grams = {}\n",
    "\n",
    "def create_mono_grams(path:str)->dict:\n",
    "    \"\"\"\n",
    "    Create a dictionary with the monograms and their counts\n",
    "    Params:\n",
    "    -------\n",
    "        path: path to the file of sentences \n",
    "    Returns:\n",
    "    --------\n",
    "        mono_grams: dictionary with the monograms and their probabilities\n",
    "    \"\"\"\n",
    "    total_sentences = 0\n",
    "    total_words = 0\n",
    "    with open(path, \"r\") as f:\n",
    "        sentence = f.readline()\n",
    "        while len(sentence) != 0:\n",
    "            words = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                total_words += 1\n",
    "                if word not in mono_grams:\n",
    "                    mono_grams[word] = 1\n",
    "                else:\n",
    "                    mono_grams[word] += 1\n",
    "            sentence = f.readline()\n",
    "            total_sentences += 1\n",
    "    mono_grams[\"<s>\"] = total_sentences\n",
    "    mono_grams[\"</s>\"] = total_sentences\n",
    "    for word in mono_grams:\n",
    "        mono_grams[word] /= total_words\n",
    "    return mono_grams\n",
    "\n",
    "news_monogram = create_mono_grams(PATH_NEWS_TRAIN)\n",
    "bac_monogram = create_mono_grams(PATH_BAC_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'</s>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luccas\\Documents\\Andes\\S9\\NLP\\Tareas\\Tarea2NLP\\src\\ConstructionLM.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m news_monogram[\u001b[39m\"\u001b[39;49m\u001b[39m</s>\u001b[39;49m\u001b[39m\"\u001b[39;49m]  \n",
      "\u001b[1;31mKeyError\u001b[0m: '</s>'"
     ]
    }
   ],
   "source": [
    "news_monogram[\"</s>\"]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
