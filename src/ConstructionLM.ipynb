{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 2 Búsqueda binaria usando índice invertido (BSII)\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo para poder hacer la busqueda binaria a travez de indice invertido es la tokenizacion de los documentos y generacion del vocabulario. Ademas es importante tener en cuenta que el vocabulario debe estar ordenado, no debe contener stop-words, debe estar stemizado y normalizado\n",
    "\n",
    "* A continucion se cargan los documentos y los queries en una estructura de datos, se debe cambiar document_path y query_path por la ruta donde se encuentran los documentos y los queries respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wes2015.d001.naf</td>\n",
       "      <td>William Beaumont and the Human Digestion.  Wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wes2015.d002.naf</td>\n",
       "      <td>Selma Lagerlöf and the wonderful Adventures of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wes2015.d003.naf</td>\n",
       "      <td>Ferdinand de Lesseps and the Suez Canal.  Ferd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wes2015.d004.naf</td>\n",
       "      <td>Walt Disney’s ‘Steamboat Willie’ and the Rise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wes2015.d005.naf</td>\n",
       "      <td>Eugene Wigner and the Structure of the Atomic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>wes2015.d327.naf</td>\n",
       "      <td>James Parkinson and Parkinson’s Disease.  Wood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>wes2015.d328.naf</td>\n",
       "      <td>Juan de la Cierva and the Autogiro.  Demonstra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>wes2015.d329.naf</td>\n",
       "      <td>Squire Whipple – The Father of the Iron Bridge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>wes2015.d330.naf</td>\n",
       "      <td>William Playfair and the Beginnings of Infogra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>wes2015.d331.naf</td>\n",
       "      <td>Juan Bautista de Anza and the Route to San Fra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename                                               body\n",
       "1    wes2015.d001.naf  William Beaumont and the Human Digestion.  Wil...\n",
       "2    wes2015.d002.naf  Selma Lagerlöf and the wonderful Adventures of...\n",
       "3    wes2015.d003.naf  Ferdinand de Lesseps and the Suez Canal.  Ferd...\n",
       "4    wes2015.d004.naf  Walt Disney’s ‘Steamboat Willie’ and the Rise ...\n",
       "5    wes2015.d005.naf  Eugene Wigner and the Structure of the Atomic ...\n",
       "..                ...                                                ...\n",
       "327  wes2015.d327.naf  James Parkinson and Parkinson’s Disease.  Wood...\n",
       "328  wes2015.d328.naf  Juan de la Cierva and the Autogiro.  Demonstra...\n",
       "329  wes2015.d329.naf  Squire Whipple – The Father of the Iron Bridge...\n",
       "330  wes2015.d330.naf  William Playfair and the Beginnings of Infogra...\n",
       "331  wes2015.d331.naf  Juan Bautista de Anza and the Route to San Fra...\n",
       "\n",
       "[331 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "\n",
    "# Rutas a definir segun la ubicacion de los archivos\n",
    "DOCUMENTS_PATH = '../data/docs-raw-texts'\n",
    "QUERIES_PATH = '../data/queries-raw-texts'\n",
    "QUERIES_RESULTS_FILE_PATH = \"../data/BSII-AND-queries_results.tsv\"\n",
    "\n",
    "def load_documents(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a Pandas DataFrame where each row represents a document in folder_path.\n",
    "    The DataFrame will have as many rows as there are documents in folder_path\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            folder_path: str\n",
    "                The path to the folder that contains the documents to load\n",
    "    \n",
    "        Returns\n",
    "        --------\n",
    "            documents: pd.DataFrame\n",
    "                Pandas DataFrame with two columns: \"filename\" and \"body\"\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    for filename in os.listdir(folder_path):\n",
    "        text = pd.read_xml(os.path.join(folder_path, filename))['raw'].tolist()[1]\n",
    "        filtered_text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "        document = [filename, filtered_text]\n",
    "        documents.append(document)\n",
    "        index.append(id)\n",
    "        id += 1\n",
    "\n",
    "    return pd.DataFrame(documents, index, columns)\n",
    "\n",
    "documents = load_documents(DOCUMENTS_PATH)\n",
    "queries = load_documents(QUERIES_PATH)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero que todo tokenizamos el texto, para esto utilizamos el word tokenize de la libreria NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents['tokens'] = documents['body'].apply(word_tokenize)\n",
    "queries['tokens'] = queries['body'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos todos los signos de puntuacion, contracciones del ingles y dejamos el texto todo en minusculas (normalizar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(token_list):\n",
    "    return [token.lower() for token in token_list if (token not in string.punctuation and (len(token)>1 or token.isnumeric()))]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_punctuation(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de tokenizar, dejar todo en minusculas, quitaremos las stop words para que reduzcan el vocabulario y no afecten el resultado final. Para esto usaremos la libreria nltk y su metodo stopwords.words('english')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#TODO no se si normalizar cuente como poner todo en minusculas\n",
    "def remove_stop_words(token_list):\n",
    "    return [token for token in token_list if token not in stop_words]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_stop_words(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de eliminar las stop words se hace stemming a las palabras restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(token_list):\n",
    "    return [stemmer.stem(token) for token in token_list]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: stemming(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto el texto de cada documento y query esta en un formato mas facil de procesar, por lo que se procede a realizar la representacion vectorial de los documentos y queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se hace la implementación para transformar el anterior dataframe en una estructura de indice inertido para así poder realizar busquedas binarias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(documents: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Creates the inverted index for a document set.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        documents: pd.DataFrame\n",
    "            A Pandas DataFrame that represents the document set. The\n",
    "            DataFrame should have the following columns: \"filename\", \"body\".\n",
    "            DataFrame's index should correspond to the document id\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        inverted_index: dict\n",
    "            A python dictionary that represents the inverted index.\n",
    "            Keys are the terms in the vocabulary.\n",
    "            Each value has a \"df\" (document frecuency) and \"postings\".\n",
    "            \"postings\" are a numpy array\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "\n",
    "    for id, document in documents.iterrows():\n",
    "        for token in document['tokens']:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {\"df\": 0, \"postings\": []}\n",
    "            if id not in inverted_index[token][\"postings\"]:\n",
    "                inverted_index[token][\"df\"] += 1\n",
    "                inverted_index[token][\"postings\"].append(id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = create_inverted_index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se pudo observar en el anterior código, se crea un diccionario que almacenara el índice invertido haciendo un recorrido por cada uno de los documentos y sus tokens. Agregando así todos los tokens del vocabulario y añadiendo a cada token el listado de documentos que contienen ese token. El vocabulario final cuenta con 14682 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_intersect(postings1: list, postings2: list) -> list:\n",
    "    \"\"\"\n",
    "    Returns the intersection of two postings lists\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    intersection = []\n",
    "\n",
    "    # Merge algorith taken from the book\n",
    "    while(i < len(postings1) and j < len(postings2)):\n",
    "        docId1 = postings1[i]\n",
    "        docId2 = postings2[j]\n",
    "        if docId1 == docId2:\n",
    "            intersection.append(docId1)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif docId1 < docId2:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return intersection\n",
    "\n",
    "\n",
    "def and_search(terms: list, inverted_index: dict) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list with the ids if the documents that\n",
    "    contain all of the terms in terms list.\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "            terms: list[str]\n",
    "                list of terms to look for in the documents\n",
    "            \n",
    "            inverted_index: dict\n",
    "                Inverted index created from the document base\n",
    "    \"\"\"\n",
    "    term_df_list = []\n",
    "\n",
    "    for term in terms:\n",
    "        if term in inverted_index:\n",
    "            term_df_list.append({\n",
    "                \"term\": term,\n",
    "                \"df\": inverted_index[term][\"df\"]\n",
    "            })\n",
    "        else:\n",
    "            # If a term that is not in the inverted index\n",
    "            # is found. That means that there is no document\n",
    "            # in the document base that meets the query.\n",
    "            # Hence, an empty array is returned\n",
    "            return []\n",
    "        \n",
    "    # If there is only one term to match, the function\n",
    "    # returns the postings of that term\n",
    "    if len(term_df_list) == 1:\n",
    "        return inverted_index[term_df_list[0][\"term\"]][\"postings\"]\n",
    "\n",
    "    # Sort term_df_list based on df\n",
    "    term_df_list.sort(key=lambda elem: elem[\"df\"])\n",
    "\n",
    "    # Initialize intersection as the smallest postings list\n",
    "    intersection = inverted_index[term_df_list[0][\"term\"]][\"postings\"]\n",
    "\n",
    "    for i in range(1, len(term_df_list)):\n",
    "        # If there are no items in the current intersection\n",
    "        # there is no point in calculating the intersection\n",
    "        # for the rest of the postings.\n",
    "        # Hence, the function returns current (empty) intersection\n",
    "        if len(intersection) == 0:\n",
    "            return intersection\n",
    "        \n",
    "        postings_i = inverted_index[term_df_list[i][\"term\"]][\"postings\"]\n",
    "\n",
    "        # calculate the intersection of current intersection with the next\n",
    "        # smallest posting list\n",
    "        intersection = and_intersect(intersection, postings_i)\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# query processing\n",
    "\n",
    "# Clear output file contents\n",
    "open(QUERIES_RESULTS_FILE_PATH, \"w\").close()\n",
    "\n",
    "# Loop through queries\n",
    "for i, query in queries.iterrows():\n",
    "    # Open output file\n",
    "    file = open(QUERIES_RESULTS_FILE_PATH, \"a\")\n",
    "    query_str = query['filename'].replace('.naf', '').replace('wes2015.', '')\n",
    "    file.write(query_str + \" \")\n",
    "\n",
    "    # Perform AND query with all the terms in the query\n",
    "    res = and_search(query[\"tokens\"], inverted_index)\n",
    "\n",
    "    # Write output file\n",
    "    for docId in res:\n",
    "        if docId < 10:\n",
    "            doc_str = \"d00\" + str(docId)\n",
    "        elif docId < 100:\n",
    "            doc_str = \"d0\" + str(docId)\n",
    "        else:\n",
    "            doc_str = \"d\" + str(docId)\n",
    "        file.write(doc_str)\n",
    "        if docId != res[len(res) - 1]:\n",
    "            file.write(\",\")\n",
    "    if query_str != \"q46\":\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
