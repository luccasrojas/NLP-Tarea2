{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construccion del modelo de lenguaje\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo es la union de todos los documentos en un solo un par de archivos, uno con los documentos de 20news y otro con lso documentos de BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m sent_tokenize, word_tokenize, WhitespaceTokenizer\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\__init__.py:146\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjsontags\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m# PACKAGES\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39m###########################################################\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchunk\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassify\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minference\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\chunk\\__init__.py:155\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39m     pattern is valid.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchunk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m ChunkParserI\n\u001b[0;32m    156\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchunk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregexp\u001b[39;00m \u001b[39mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n\u001b[0;32m    157\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchunk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    158\u001b[0m     ChunkScore,\n\u001b[0;32m    159\u001b[0m     accuracy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     tree2conlltags,\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\chunk\\api.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Copyright (C) 2001-2023 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m##  Chunk Parser Interface\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchunk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m ChunkScore\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternals\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m \u001b[39mimport\u001b[39;00m ParserI\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\chunk\\util.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy \u001b[39mas\u001b[39;00m _accuracy\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m map_tag\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m str2tuple\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m Tree\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\tag\\__init__.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m TaggerI\n\u001b[0;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m str2tuple, tuple2str, untag\n\u001b[1;32m---> 70\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     71\u001b[0m     SequentialBackoffTagger,\n\u001b[0;32m     72\u001b[0m     ContextTagger,\n\u001b[0;32m     73\u001b[0m     DefaultTagger,\n\u001b[0;32m     74\u001b[0m     NgramTagger,\n\u001b[0;32m     75\u001b[0m     UnigramTagger,\n\u001b[0;32m     76\u001b[0m     BigramTagger,\n\u001b[0;32m     77\u001b[0m     TrigramTagger,\n\u001b[0;32m     78\u001b[0m     AffixTagger,\n\u001b[0;32m     79\u001b[0m     RegexpTagger,\n\u001b[0;32m     80\u001b[0m     ClassifierBasedTagger,\n\u001b[0;32m     81\u001b[0m     ClassifierBasedPOSTagger,\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbrill\u001b[39;00m \u001b[39mimport\u001b[39;00m BrillTagger\n\u001b[0;32m     84\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbrill_trainer\u001b[39;00m \u001b[39mimport\u001b[39;00m BrillTaggerTrainer\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\tag\\sequential.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Optional, Tuple\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m jsontags\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassify\u001b[39;00m \u001b[39mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m ConditionalFreqDist\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m FeaturesetTaggerI, TaggerI\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\classify\\__init__.py:97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassify\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpositivenaivebayes\u001b[39;00m \u001b[39mimport\u001b[39;00m PositiveNaiveBayesClassifier\n\u001b[0;32m     96\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassify\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrte_classify\u001b[39;00m \u001b[39mimport\u001b[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001b[1;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassify\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscikitlearn\u001b[39;00m \u001b[39mimport\u001b[39;00m SklearnClassifier\n\u001b[0;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassify\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msenna\u001b[39;00m \u001b[39mimport\u001b[39;00m Senna\n\u001b[0;32m     99\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassify\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtextcat\u001b[39;00m \u001b[39mimport\u001b[39;00m TextCat\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\nltk\\classify\\scikitlearn.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobability\u001b[39;00m \u001b[39mimport\u001b[39;00m DictionaryProbDist\n\u001b[0;32m     37\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m \u001b[39mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m     39\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\sklearn\\feature_extraction\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.feature_extraction` module deals with feature extraction\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom raw data. It currently includes methods to extract features from text and\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimages.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m text\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_dict_vectorizer\u001b[39;00m \u001b[39mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_hash\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureHasher\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m HasMethods, Interval, RealNotInt, StrOptions\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m FLOAT_DTYPES, check_array, check_is_fitted\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_hash\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureHasher\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stop_words\u001b[39;00m \u001b[39mimport\u001b[39;00m ENGLISH_STOP_WORDS\n\u001b[0;32m     36\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHashingVectorizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCountVectorizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstrip_tags\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\juanc\\anaconda3\\envs\\nlpEnv\\Lib\\site-packages\\sklearn\\feature_extraction\\_hash.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEstimator, TransformerMixin, _fit_context\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, StrOptions\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_hashing_fast\u001b[39;00m \u001b[39mimport\u001b[39;00m transform \u001b[39mas\u001b[39;00m _hashing_transform\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iteritems\u001b[39m(d):\n\u001b[0;32m     16\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Like d.iteritems, but accepts any collections.Mapping.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32msklearn\\feature_extraction\\_hashing_fast.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.feature_extraction._hashing_fast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continucion se recorren todos los documentos de ambas carpetas y se unen en un solo archivo.\n",
    "* Se debe modificar la ruta de los archivos para que se ajuste. La ruta de la carpeta de 20news debe estar en PATH_20NEWS y la ruta de la carpeta de BAC debe estar en PATH_BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_20NEWS = '../data/20news-18828'\n",
    "PATH_FINAL_20NEWS = \"../data/final_20news.txt\"\n",
    "\n",
    "def load_news(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "\n",
    "    \"\"\"\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\") as final_document:\n",
    "        for folder in os.listdir(documents_path):\n",
    "            for document_file in os.listdir(os.path.join(documents_path, folder)):\n",
    "                with open(os.path.join(documents_path, folder, document_file), \"r\") as document:\n",
    "                    text = document.read()\n",
    "                final_document.write(text)\n",
    "                final_document.write(\"\\n\")\n",
    "\n",
    "# load_news() should only be called if the PATH_FINAL_NEWS does not\n",
    "# exist yet.\n",
    "if not os.path.isfile(PATH_FINAL_20NEWS):\n",
    "    load_news(PATH_20NEWS,PATH_FINAL_20NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PATH_BAC= '../data/BAC/blogs/blogs'\n",
    "PATH_FINAL_BAC = \"../data/final_bac.txt\"\n",
    "\n",
    "def load_bac(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "    \"\"\"\n",
    "    pattern = r'<post>(.*?)</post>'\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\", encoding = 'latin_1') as final_document:\n",
    "        for file_name in os.listdir(documents_path):\n",
    "            with open(os.path.join(PATH_BAC,file_name) , encoding=\"latin_1\") as f:\n",
    "                text = f.read()\n",
    "                texts = re.findall(pattern, text, re.DOTALL)\n",
    "            all_text= \". \\n\".join(texts)\n",
    "            filtered_text = all_text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "            final_document.write(filtered_text)\n",
    "\n",
    "# load_bac() should only be called if the PATH_FINAL_BAC does not\n",
    "# exist yet.\n",
    "if not os.path.isfile(PATH_FINAL_BAC):\n",
    "    load_bac(PATH_BAC,PATH_FINAL_BAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     raw_news \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(PATH_FINAL_BAC, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m     raw_bac \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread()\n\u001b[0;32m      6\u001b[0m news_sentences \u001b[39m=\u001b[39m sent_tokenize(raw_news)\n\u001b[0;32m      7\u001b[0m bac_sentences \u001b[39m=\u001b[39m sent_tokenize(raw_bac)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(PATH_FINAL_20NEWS, \"r\", errors='ignore') as f:\n",
    "    raw_news = f.read()\n",
    "with open(PATH_FINAL_BAC, \"r\", errors='ignore') as f:\n",
    "    raw_bac = f.read()\n",
    "\n",
    "news_sentences = sent_tokenize(raw_news)\n",
    "bac_sentences = sent_tokenize(raw_bac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente se leen los archivos y se normalizan con el formato adecuado para el modelo de lenguaje. De este modo se pone todo el minusculas, se remplazan los numeros por num y se agregan caracteres al inicio y al final de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence:str)->str:\n",
    "    \"\"\"\n",
    "    Normalize a sentence by lowercasing it, replacing numbers with NUM and adding <s> and </s> tokens\n",
    "    Params:\n",
    "    -------\n",
    "        sentence: sentence to normalize\n",
    "    Returns:\n",
    "    --------\n",
    "        sentence: normalized sentence\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = re.sub(r\"[^(a-zA-Z0-9\\s)]\", \" \", sentence).lower().replace(\"\\n\", \" \")\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    for word in words:\n",
    "        try:\n",
    "            word\n",
    "            number = float(word)\n",
    "            sentence = sentence.replace(word, \"NUM\")\n",
    "        except:\n",
    "            pass\n",
    "    sentence = f\"<s> {sentence} </s>\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego se extrae el vocabulario de todas las palabras junto con su frecuencia para asi poder reemplazar los tokens que no se encuentren en el vocabulario por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(sentences:list)->dict:\n",
    "    \"\"\"\n",
    "    Extract the vocabulary from a list of sentences\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "    Returns:\n",
    "    --------\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    for sentence in sentences:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = 1\n",
    "            else:   \n",
    "                vocabulary[word] += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con el vocabulario y las frases se cambian todos los elementos que aparecen sola una vez en el corpus por el token \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknowns(sentences:list, vocabulary:dict)->list:\n",
    "    \"\"\"\n",
    "    Replace the words that appear only once in the corpus by the <UNK> token\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    Returns:\n",
    "    --------\n",
    "        sentences: list of sentences with the <UNK> token\n",
    "    \"\"\"\n",
    "    vocabulary[\"<UNK>\"] = 0\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for j, word in enumerate(words):\n",
    "            if vocabulary[word] == 1:\n",
    "                sentences[i] = sentence.replace(word, \"<UNK>\")\n",
    "                vocabulary[\"<UNK>\"] += 1\n",
    "                del vocabulary[word]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este punto se normalizan los 2 sets y se extrae su vocabulario para si poder remplazar los tokens que se encuentren en el vocabulario solo una vez por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> from  mathew  mathew mantis co uk \n",
      "subject  alt atheism faq  atheist resources\n",
      "\n",
      "archive name  atheism resources\n",
      "alt atheism archive name  resources\n",
      "last modified  NUM december NUM\n",
      "version  NUM NUM\n",
      "\n",
      "                              atheist resources\n",
      "\n",
      "                      addresses of atheist organizations\n",
      "\n",
      "                                     usa\n",
      "\n",
      "freedom from religion foundation\n",
      "\n",
      "darwin fish bumper stickers and assorted other atheist paraphernalia are\n",
      "available from the freedom from religion foundation in the us  </s>\n"
     ]
    }
   ],
   "source": [
    "normalized_news_sentences = [normalize(sentence) for sentence in news_sentences]\n",
    "normalized_bac_sentences = [normalize(sentence) for sentence in bac_sentences]\n",
    "print(normalized_news_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m news_vocabulary \u001b[39m=\u001b[39m extract_vocabulary(normalized_news_sentences)\n\u001b[1;32m----> 2\u001b[0m bac_vocabulary \u001b[39m=\u001b[39m extract_vocabulary(normalized_bac_sentences)\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mextract_vocabulary\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     16\u001b[0m             vocabulary[word] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     17\u001b[0m         \u001b[39melse\u001b[39;00m:   \n\u001b[1;32m---> 18\u001b[0m             vocabulary[word] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m vocabulary\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "news_vocabulary = extract_vocabulary(normalized_news_sentences)\n",
    "bac_vocabulary = extract_vocabulary(normalized_bac_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sentences = replace_unknowns(normalized_news_sentences, news_vocabulary)\n",
    "bac_sentences = replace_unknowns(normalized_bac_sentences, bac_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "news_train, news_test = train_test_split(news_sentences, test_size=0.20)\n",
    "bac_train, bac_test = train_test_split(bac_sentences, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vocabulary = extract_vocabulary(news_train)\n",
    "bac_vocabulary = extract_vocabulary(bac_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego guardamos estos datos en 4 archivos, 2 para train y 2 para test, uno para 20news y otro para BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_TRAIN = \"../data/20N_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_NEWS_TEST = \"../data/20N_l.rojasb_j.arboleda_test.txt\"  \n",
    "PATH_BAC_TRAIN = \"../data/BAC_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_BAC_TEST = \"../data/BAC_l.rojasb_j.arboleda_test.txt\" \n",
    "\n",
    "def save_file(sentences:list, path:str):\n",
    "    \"\"\"\n",
    "    Save a list of sentences in a file\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        path: path to the file\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "save_file(news_train, PATH_NEWS_TRAIN)\n",
    "save_file(news_test, PATH_NEWS_TEST)\n",
    "save_file(bac_train, PATH_BAC_TRAIN)\n",
    "save_file(bac_test, PATH_BAC_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego pasamos a crear los n-gramas, para esto construimos diccionarios para cada combinacion posible de n-gramas, en este caso se construyen diccionario para los monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_UNIGRAM=\"../data/20N_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "PATH_BAC_UNIGRAM=\"../data/BAC_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "\n",
    "def create_uni_grams(path: str, save_path: str)->dict:\n",
    "    \"\"\"\n",
    "    Creates and saves the monograms\n",
    "    \"\"\"\n",
    "    mono_grams = {}\n",
    "    total_words = 0\n",
    "    f = open(path, \"r\")\n",
    "    sf = open(path, \"w\")\n",
    "\n",
    "    sentence = f.readline()\n",
    "    while len(sentence) != 0:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for word in words:\n",
    "            total_words += 1\n",
    "            if word not in mono_grams:\n",
    "                mono_grams[word] = 1\n",
    "            else:\n",
    "                mono_grams[word] += 1\n",
    "        sentence = f.readline()\n",
    "            \n",
    "    for word in mono_grams:\n",
    "        prob = mono_grams[word] / total_words\n",
    "        sf.write(f\"{word},{mono_grams[word]},{prob}\")\n",
    "\n",
    "create_uni_grams(PATH_NEWS_TRAIN, PATH_NEWS_UNIGRAM)\n",
    "create_uni_grams(PATH_BAC_TRAIN, PATH_BAC_UNIGRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente podemos guardar los diccionarios del unigrama en archivos para poder cargarlos posterioremtne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aca creamos los diccionarios para los bigramas y los guardamos en archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_BIGRAM=\"../data/20N_l.rojasb_j.arboleda_bigrams.txt\"\n",
    "PATH_BAC_BIGRAM=\"../data/BAC_l.rojasb_j.arboleda_bigrams.txt\"\n",
    "\n",
    "def create_bi_grams(sentence_path:str, vocabulary:dict, save_path: str)->dict:\n",
    "    \"\"\" \n",
    "    This function creates and saves a bigram. Returns the bigram count\n",
    "    \"\"\"\n",
    "    bigram_count = {}\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    f = open(sentence_path, \"r\")\n",
    "    sf = open(save_path, \"w\")\n",
    "    \n",
    "    sentence = f.readline()\n",
    "    while len(sentence) != 0:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for i in range(len(words)-1):\n",
    "            key = words[i] + \" \" + words[i+1]\n",
    "            if key not in bigram_count:\n",
    "                bigram_count[key] = 1\n",
    "            else:\n",
    "                bigram_count[key] += 1\n",
    "        sentence = f.readline()\n",
    "\n",
    "    for bigram in bigram_count:\n",
    "        words = bigram.split(\" \")\n",
    "        key = words[0] + \" \" + words[1]\n",
    "        prob = (bigram_count[key]+1)/(vocabulary[words[0]] + vocab_size)\n",
    "        sf.write(f\"{bigram},{bigram_count[key]},{prob}\\n\")\n",
    "            \n",
    "    return bigram_count\n",
    "\n",
    "news_bigrams_count = create_bi_grams(PATH_NEWS_TRAIN, news_vocabulary, PATH_NEWS_BIGRAM)\n",
    "bac_bigrams_count = create_bi_grams(PATH_BAC_TRAIN, bac_vocabulary, PATH_BAC_BIGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "PATH_NEWS_TRIGRAM=\"../data/20N_l.rojasb_j.arboleda_trigrams.txt\"\n",
    "PATH_BAC_TRIGRAM=\"../data/BAC_l.rojasb_j.arboleda_trigrams.txt\"\n",
    "\n",
    "def create_tri_grams(sentence_path: str, vocabulary: dict, bi_grams_count: dict, save_path: str)->dict:\n",
    "    \"\"\" \n",
    "    Creates and saves the trigrams\n",
    "    \"\"\"\n",
    "    trigrams_count={}\n",
    "\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    f = open(sentence_path, \"r\")\n",
    "    sf = open(save_path, \"w\")\n",
    "\n",
    "    sentence = f.readline()\n",
    "    while len(sentence) != 0:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for i in range(len(words)-2):\n",
    "            key = words[i] + \" \" + words[i+1] + \" \" + words[i+2]\n",
    "            if key not in trigrams_count:\n",
    "                trigrams_count[key] = 1\n",
    "            else:\n",
    "                trigrams_count[key] += 1\n",
    "        sentence = f.readline()\n",
    "\n",
    "    for tri_gram in trigrams_count:\n",
    "        words = tri_gram.split(\" \")\n",
    "        key = words[0] + \" \" + words[1] + \" \" + words[2]\n",
    "        prob = (trigrams_count[key] + 1) / (bi_grams_count[words[0] + \" \" + words[1]] + vocab_size^2)\n",
    "        sf.write(f\"{tri_gram},{trigrams_count[key]},{prob}\\n\")\n",
    "\n",
    "create_tri_grams(PATH_NEWS_TRAIN, news_vocabulary, news_bigrams_count, PATH_NEWS_TRIGRAM)\n",
    "print(\"done\")\n",
    "create_tri_grams(PATH_BAC_TRAIN, bac_vocabulary, bac_bigrams_count, PATH_BAC_TRIGRAM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
