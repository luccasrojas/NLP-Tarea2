{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construccion del modelo de lenguaje\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo es la union de todos los documentos en un solo un par de archivos, uno con los documentos de 20news y otro con lso documentos de BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continucion se recorren todos los documentos de ambas carpetas y se unen en un solo archivo.\n",
    "* Se debe modificar la ruta de los archivos para que se ajuste. La ruta de la carpeta de 20news debe estar en PATH_20NEWS y la ruta de la carpeta de BAC debe estar en PATH_BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_20NEWS = '../data/20news-18828'\n",
    "PATH_FINAL_20NEWS = \"../data/final_20news.txt\"\n",
    "\n",
    "def load_news(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "\n",
    "    \"\"\"\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\") as final_document:\n",
    "        for folder in os.listdir(documents_path):\n",
    "            for document_file in os.listdir(os.path.join(documents_path, folder)):\n",
    "                with open(os.path.join(documents_path, folder, document_file), \"r\") as document:\n",
    "                    text = document.read()\n",
    "                final_document.write(text)\n",
    "                final_document.write(\"\\n\")\n",
    "\n",
    "load_news(PATH_20NEWS,PATH_FINAL_20NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PATH_BAC= '../data/BAC/blogs/blogs'\n",
    "PATH_FINAL_BAC = \"../data/final_bac.txt\"\n",
    "\n",
    "def load_bac(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "    \"\"\"\n",
    "    pattern = r'<post>(.*?)</post>'\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\", encoding = 'latin_1') as final_document:\n",
    "        for file_name in os.listdir(documents_path):\n",
    "            with open(os.path.join(PATH_BAC,file_name) , encoding=\"latin_1\") as f:\n",
    "                text = f.read()\n",
    "                texts = re.findall(pattern, text, re.DOTALL)\n",
    "            all_text= \". \\n\".join(texts)\n",
    "            filtered_text = all_text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "            final_document.write(filtered_text)\n",
    "\n",
    "\n",
    "load_bac(PATH_BAC,PATH_FINAL_BAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_FINAL_20NEWS, \"r\") as f:\n",
    "    raw_news = f.read()\n",
    "with open(PATH_FINAL_20NEWS, \"r\") as f:\n",
    "    raw_bac = f.read()\n",
    "\n",
    "news_sentences = sent_tokenize(raw_news)\n",
    "bac_sentences = sent_tokenize(raw_bac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente se leen los archivos y se normalizan con el formato adecuado para el modelo de lenguaje. De este modo se pone todo el minusculas, se remplazan los numeros por num y se agregan caracteres al inicio y al final de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence:str)->str:\n",
    "    \"\"\"\n",
    "    Normalize a sentence by lowercasing it, replacing numbers with NUM and adding <s> and </s> tokens\n",
    "    Params:\n",
    "    -------\n",
    "        sentence: sentence to normalize\n",
    "    Returns:\n",
    "    --------\n",
    "        sentence: normalized sentence\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower().replace (\"\\n\", \" \")\n",
    "    words = word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        try:\n",
    "            word.replace(\",\",\"\").replace(\".\",\"\").replace(\"-\",\"\").replace(\"$\",\"\").replace(\"'\",\"\")\n",
    "            number = float(word)\n",
    "            sentence = sentence.replace(word, \"NUM\")\n",
    "        except:\n",
    "            pass\n",
    "    sentence = f\"<s> {sentence} </s>\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego se extrae el vocabulario de todas las palabras junto con su frecuencia para asi poder reemplazar los tokens que no se encuentren en el vocabulario por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(sentences:list)->dict:\n",
    "    \"\"\"\n",
    "    Extract the vocabulary from a list of sentences\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "    Returns:\n",
    "    --------\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = 1\n",
    "            else:   \n",
    "                vocabulary[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con el vocabulario y las frases se cambian todos los elementos que aparecen sola una vez en el corpus por el token \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknowns(sentences:list, vocabulary:dict)->list:\n",
    "    \"\"\"\n",
    "    Replace the words that appear only once in the corpus by the <UNK> token\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    Returns:\n",
    "    --------\n",
    "        sentences: list of sentences with the <UNK> token\n",
    "    \"\"\"\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = word_tokenize(sentence)\n",
    "        for j, word in enumerate(words):\n",
    "            try:\n",
    "                if vocabulary[word] == 1:\n",
    "                    words[j] = \"<UNK>\"\n",
    "            except:\n",
    "                words[j] = \"<UNK>\"\n",
    "            sentences[i] = \" \".join(words)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este punto se normalizan los 2 sets y se extrae su vocabulario para si poder remplazar los tokens que se encuentren en el vocabulario solo una vez por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_news_sentences = [normalize(sentence) for sentence in news_sentences]\n",
    "normalized_bac_sentences = [normalize(sentence) for sentence in bac_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vocabulary = extract_vocabulary(normalized_news_sentences)\n",
    "bac_vocabulary = extract_vocabulary(normalized_bac_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m news_sentences \u001b[39m=\u001b[39m replace_unknowns(normalized_news_sentences, news_vocabulary)\n\u001b[0;32m      2\u001b[0m bac_sentences \u001b[39m=\u001b[39m replace_unknowns(normalized_bac_sentences, bac_vocabulary)\n",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m, in \u001b[0;36mreplace_unknowns\u001b[1;34m(sentences, vocabulary)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(words) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m j, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words):\n\u001b[1;32m---> 16\u001b[0m         \u001b[39mif\u001b[39;00m vocabulary[word] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     17\u001b[0m             words[j] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<UNK>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m     sentences[i] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(words)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "news_sentences = replace_unknowns(normalized_news_sentences, news_vocabulary)\n",
    "bac_sentences = replace_unknowns(normalized_bac_sentences, bac_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> from: mathew <mathew@mantis.co.uk> subject: alt.atheism faq: atheist resources  archive-name: atheism/resources alt-atheism-archive-name: resources last-modified: NUM december NUM version: NUM                                atheist resources                        addresses of atheist organizations                                       usa  freedom from religion foundation  darwin fish bumper stickers and assorted other atheist paraphernalia are available from the freedom from religion foundation in the us. </s>\n"
     ]
    }
   ],
   "source": [
    "print(normalized_news_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luccas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mpunkt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m documents[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m documents[\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(word_tokenize)\n\u001b[0;32m      5\u001b[0m queries[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m queries[\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(word_tokenize)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "documents['tokens'] = documents['body'].apply(word_tokenize)\n",
    "queries['tokens'] = queries['body'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removemos todos los signos de puntuacion, contracciones del ingles y dejamos el texto todo en minusculas (normalizar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(token_list):\n",
    "    return [token.lower() for token in token_list if (token not in string.punctuation and (len(token)>1 or token.isnumeric()))]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_punctuation(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de tokenizar, dejar todo en minusculas, quitaremos las stop words para que reduzcan el vocabulario y no afecten el resultado final. Para esto usaremos la libreria nltk y su metodo stopwords.words('english')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#TODO no se si normalizar cuente como poner todo en minusculas\n",
    "def remove_stop_words(token_list):\n",
    "    return [token for token in token_list if token not in stop_words]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: remove_stop_words(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de eliminar las stop words se hace stemming a las palabras restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stemming(token_list):\n",
    "    return [stemmer.stem(token) for token in token_list]\n",
    "\n",
    "documents['tokens']=documents['tokens'].apply(lambda x: stemming(x))\n",
    "queries['tokens']=queries['tokens'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto el texto de cada documento y query esta en un formato mas facil de procesar, por lo que se procede a realizar la representacion vectorial de los documentos y queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se hace la implementación para transformar el anterior dataframe en una estructura de indice inertido para así poder realizar busquedas binarias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(documents: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Creates the inverted index for a document set.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        documents: pd.DataFrame\n",
    "            A Pandas DataFrame that represents the document set. The\n",
    "            DataFrame should have the following columns: \"filename\", \"body\".\n",
    "            DataFrame's index should correspond to the document id\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        inverted_index: dict\n",
    "            A python dictionary that represents the inverted index.\n",
    "            Keys are the terms in the vocabulary.\n",
    "            Each value has a \"df\" (document frecuency) and \"postings\".\n",
    "            \"postings\" are a numpy array\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "\n",
    "    for id, document in documents.iterrows():\n",
    "        for token in document['tokens']:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {\"df\": 0, \"postings\": []}\n",
    "            if id not in inverted_index[token][\"postings\"]:\n",
    "                inverted_index[token][\"df\"] += 1\n",
    "                inverted_index[token][\"postings\"].append(id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = create_inverted_index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se pudo observar en el anterior código, se crea un diccionario que almacenara el índice invertido haciendo un recorrido por cada uno de los documentos y sus tokens. Agregando así todos los tokens del vocabulario y añadiendo a cada token el listado de documentos que contienen ese token. El vocabulario final cuenta con 14682 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_intersect(postings1: list, postings2: list) -> list:\n",
    "    \"\"\"\n",
    "    Returns the intersection of two postings lists\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    intersection = []\n",
    "\n",
    "    # Merge algorith taken from the book\n",
    "    while(i < len(postings1) and j < len(postings2)):\n",
    "        docId1 = postings1[i]\n",
    "        docId2 = postings2[j]\n",
    "        if docId1 == docId2:\n",
    "            intersection.append(docId1)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif docId1 < docId2:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return intersection\n",
    "\n",
    "\n",
    "def and_search(terms: list, inverted_index: dict) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list with the ids if the documents that\n",
    "    contain all of the terms in terms list.\n",
    "\n",
    "        Params\n",
    "        ------\n",
    "            terms: list[str]\n",
    "                list of terms to look for in the documents\n",
    "            \n",
    "            inverted_index: dict\n",
    "                Inverted index created from the document base\n",
    "    \"\"\"\n",
    "    term_df_list = []\n",
    "\n",
    "    for term in terms:\n",
    "        if term in inverted_index:\n",
    "            term_df_list.append({\n",
    "                \"term\": term,\n",
    "                \"df\": inverted_index[term][\"df\"]\n",
    "            })\n",
    "        else:\n",
    "            # If a term that is not in the inverted index\n",
    "            # is found. That means that there is no document\n",
    "            # in the document base that meets the query.\n",
    "            # Hence, an empty array is returned\n",
    "            return []\n",
    "        \n",
    "    # If there is only one term to match, the function\n",
    "    # returns the postings of that term\n",
    "    if len(term_df_list) == 1:\n",
    "        return inverted_index[term_df_list[0][\"term\"]][\"postings\"]\n",
    "\n",
    "    # Sort term_df_list based on df\n",
    "    term_df_list.sort(key=lambda elem: elem[\"df\"])\n",
    "\n",
    "    # Initialize intersection as the smallest postings list\n",
    "    intersection = inverted_index[term_df_list[0][\"term\"]][\"postings\"]\n",
    "\n",
    "    for i in range(1, len(term_df_list)):\n",
    "        # If there are no items in the current intersection\n",
    "        # there is no point in calculating the intersection\n",
    "        # for the rest of the postings.\n",
    "        # Hence, the function returns current (empty) intersection\n",
    "        if len(intersection) == 0:\n",
    "            return intersection\n",
    "        \n",
    "        postings_i = inverted_index[term_df_list[i][\"term\"]][\"postings\"]\n",
    "\n",
    "        # calculate the intersection of current intersection with the next\n",
    "        # smallest posting list\n",
    "        intersection = and_intersect(intersection, postings_i)\n",
    "\n",
    "    return intersection\n",
    "\n",
    "# query processing\n",
    "\n",
    "# Clear output file contents\n",
    "open(QUERIES_RESULTS_FILE_PATH, \"w\").close()\n",
    "\n",
    "# Loop through queries\n",
    "for i, query in queries.iterrows():\n",
    "    # Open output file\n",
    "    file = open(QUERIES_RESULTS_FILE_PATH, \"a\")\n",
    "    query_str = query['filename'].replace('.naf', '').replace('wes2015.', '')\n",
    "    file.write(query_str + \" \")\n",
    "\n",
    "    # Perform AND query with all the terms in the query\n",
    "    res = and_search(query[\"tokens\"], inverted_index)\n",
    "\n",
    "    # Write output file\n",
    "    for docId in res:\n",
    "        if docId < 10:\n",
    "            doc_str = \"d00\" + str(docId)\n",
    "        elif docId < 100:\n",
    "            doc_str = \"d0\" + str(docId)\n",
    "        else:\n",
    "            doc_str = \"d\" + str(docId)\n",
    "        file.write(doc_str)\n",
    "        if docId != res[len(res) - 1]:\n",
    "            file.write(\",\")\n",
    "    if query_str != \"q46\":\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
