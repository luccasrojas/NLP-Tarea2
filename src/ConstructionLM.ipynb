{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construccion del modelo de lenguaje\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento\n",
    "Lo primero que se llevara a cabo es la union de todos los documentos en un solo un par de archivos, uno con los documentos de 20news y otro con lso documentos de BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continucion se recorren todos los documentos de ambas carpetas y se unen en un solo archivo.\n",
    "* Se debe modificar la ruta de los archivos para que se ajuste. La ruta de la carpeta de 20news debe estar en PATH_20NEWS y la ruta de la carpeta de BAC debe estar en PATH_BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_20NEWS = '../data/20news-18828'\n",
    "PATH_FINAL_20NEWS = \"../data/final_20news.txt\"\n",
    "\n",
    "def load_news(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "\n",
    "    \"\"\"\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\") as final_document:\n",
    "        for folder in os.listdir(documents_path):\n",
    "            for document_file in os.listdir(os.path.join(documents_path, folder)):\n",
    "                with open(os.path.join(documents_path, folder, document_file), \"r\") as document:\n",
    "                    text = document.read()\n",
    "                final_document.write(text)\n",
    "                final_document.write(\"\\n\")\n",
    "\n",
    "load_news(PATH_20NEWS,PATH_FINAL_20NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PATH_BAC= '../data/BAC/blogs/blogs'\n",
    "PATH_FINAL_BAC = \"../data/final_bac.txt\"\n",
    "\n",
    "def load_bac(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "    \"\"\"\n",
    "    pattern = r'<post>(.*?)</post>'\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\", encoding = 'latin_1') as final_document:\n",
    "        for file_name in os.listdir(documents_path):\n",
    "            with open(os.path.join(PATH_BAC,file_name) , encoding=\"latin_1\") as f:\n",
    "                text = f.read()\n",
    "                texts = re.findall(pattern, text, re.DOTALL)\n",
    "            all_text= \". \\n\".join(texts)\n",
    "            filtered_text = all_text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "            final_document.write(filtered_text)\n",
    "\n",
    "\n",
    "load_bac(PATH_BAC,PATH_FINAL_BAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_FINAL_20NEWS, \"r\") as f:\n",
    "    raw_news = f.read()\n",
    "with open(PATH_FINAL_20NEWS, \"r\") as f:\n",
    "    raw_bac = f.read()\n",
    "\n",
    "news_sentences = sent_tokenize(raw_news)\n",
    "bac_sentences = sent_tokenize(raw_bac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente se leen los archivos y se normalizan con el formato adecuado para el modelo de lenguaje. De este modo se pone todo el minusculas, se remplazan los numeros por num y se agregan caracteres al inicio y al final de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence:str)->str:\n",
    "    \"\"\"\n",
    "    Normalize a sentence by lowercasing it, replacing numbers with NUM and adding <s> and </s> tokens\n",
    "    Params:\n",
    "    -------\n",
    "        sentence: sentence to normalize\n",
    "    Returns:\n",
    "    --------\n",
    "        sentence: normalized sentence\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower().replace(\"\\n\", \" \").replace(\",\",\"\").replace(\".\",\"\").replace(\"-\",\"\").replace(\"$\",\"\").replace(\"'\",\"\").replace(\":\",\"\").replace(\"|\",\"\").replace(\">\",\"\").replace(\"<\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"=\",\"\").replace(\"*\",\"\")\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    for word in words:\n",
    "        try:\n",
    "            word.replace(\",\",\"\").replace(\".\",\"\").replace(\"-\",\"\").replace(\"$\",\"\").replace(\"'\",\"\")\n",
    "            number = float(word)\n",
    "            sentence = sentence.replace(word, \"NUM\")\n",
    "        except:\n",
    "            pass\n",
    "    sentence = f\"<s> {sentence} </s>\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego se extrae el vocabulario de todas las palabras junto con su frecuencia para asi poder reemplazar los tokens que no se encuentren en el vocabulario por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(sentences:list)->dict:\n",
    "    \"\"\"\n",
    "    Extract the vocabulary from a list of sentences\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "    Returns:\n",
    "    --------\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    for sentence in sentences:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = 1\n",
    "            else:   \n",
    "                vocabulary[word] += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con el vocabulario y las frases se cambian todos los elementos que aparecen sola una vez en el corpus por el token \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknowns(sentences:list, vocabulary:dict)->list:\n",
    "    \"\"\"\n",
    "    Replace the words that appear only once in the corpus by the <UNK> token\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    Returns:\n",
    "    --------\n",
    "        sentences: list of sentences with the <UNK> token\n",
    "    \"\"\"\n",
    "    vocabulary[\"<UNK>\"] = 0\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for j, word in enumerate(words):\n",
    "            if vocabulary[word] == 1:\n",
    "                sentences[i] = sentence.replace(word, \"<UNK>\")\n",
    "                vocabulary[\"<UNK>\"] += 1\n",
    "                del vocabulary[word]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este punto se normalizan los 2 sets y se extrae su vocabulario para si poder remplazar los tokens que se encuentren en el vocabulario solo una vez por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_news_sentences = [normalize(sentence) for sentence in news_sentences]\n",
    "normalized_bac_sentences = [normalize(sentence) for sentence in bac_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vocabulary = extract_vocabulary(normalized_news_sentences)\n",
    "bac_vocabulary = extract_vocabulary(normalized_bac_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sentences = replace_unknowns(normalized_news_sentences, news_vocabulary)\n",
    "bac_sentences = replace_unknowns(normalized_bac_sentences, bac_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "news_train, news_test = train_test_split(news_sentences, test_size=0.20)\n",
    "bac_train, bac_test = train_test_split(bac_sentences, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego guardamos estos datos en 4 archivos, 2 para train y 2 para test, uno para 20news y otro para BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_TRAIN = \"../data/20N_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_NEWS_TEST = \"../data/20N_l.rojasb_j.arboleda_test.txt\"  \n",
    "PATH_BAC_TRAIN = \"../data/BAC_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_BAC_TEST = \"../data/BAC_l.rojasb_j.arboleda_test.txt\" \n",
    "\n",
    "def save_file(sentences:list, path:str):\n",
    "    \"\"\"\n",
    "    Save a list of sentences in a file\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        path: path to the file\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "save_file(news_train, PATH_NEWS_TRAIN)\n",
    "save_file(news_test, PATH_NEWS_TEST)\n",
    "save_file(bac_train, PATH_BAC_TRAIN)\n",
    "save_file(bac_test, PATH_BAC_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego pasamos a crear los n-gramas, para esto construimos diccionarios para cada combinacion posible de n-gramas, en este caso se construyen diccionario para los monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_grams = {}\n",
    "\n",
    "def create_uni_grams(path:str)->dict:\n",
    "    \"\"\"\n",
    "    Create a dictionary with the monograms and their counts\n",
    "    Params:\n",
    "    -------\n",
    "        path: path to the file of sentences \n",
    "    Returns:\n",
    "    --------\n",
    "        mono_grams: dictionary with the monograms and their probabilities\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    with open(path, \"r\") as f:\n",
    "        sentence = f.readline()\n",
    "        while len(sentence) != 0:\n",
    "            words = tokenizer.tokenize(sentence)\n",
    "            for word in words:\n",
    "                total_words += 1\n",
    "                if word not in mono_grams:\n",
    "                    mono_grams[word] = 1\n",
    "                else:\n",
    "                    mono_grams[word] += 1\n",
    "            sentence = f.readline()\n",
    "            \n",
    "    for word in mono_grams:\n",
    "        mono_grams[word] /= total_words\n",
    "    return mono_grams\n",
    "\n",
    "news_monogram = create_uni_grams(PATH_NEWS_TRAIN)\n",
    "bac_monogram = create_uni_grams(PATH_BAC_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente podemos guardar los diccionarios del unigrama en archivos para poder cargarlos posterioremtne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_UNIGRAM=\"../data/20N_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "PATH_BAC_UNIGRAM=\"../data/BAC_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "\n",
    "def save_unigram(unigram:dict,path:str):\n",
    "    \"\"\"\n",
    "    Save a dictionary of monograms in a file\n",
    "    Params:\n",
    "    -------\n",
    "        unigram: dictionary of monograms\n",
    "        path: path to the file\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        for word in unigram:\n",
    "            f.write(f\"{word},{unigram[word]}\\n\")\n",
    "\n",
    "save_unigram(news_monogram,PATH_NEWS_UNIGRAM)\n",
    "save_unigram(bac_monogram,PATH_BAC_UNIGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'#601821'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luccas\\Documents\\Andes\\S9\\NLP\\Tareas\\Tarea2NLP\\src\\ConstructionLM.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m bi_grams\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# news_bigram = create_bi_grams(PATH_NEWS_TRAIN, news_vocabulary)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m bac_bigram \u001b[39m=\u001b[39m create_bi_grams(PATH_BAC_TRAIN, bac_vocabulary)\n",
      "\u001b[1;32mc:\\Users\\luccas\\Documents\\Andes\\S9\\NLP\\Tareas\\Tarea2NLP\\src\\ConstructionLM.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m words \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(sentence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(words)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m words[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m bi_grams[words[i]]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         bi_grams[words[i]][words[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luccas/Documents/Andes/S9/NLP/Tareas/Tarea2NLP/src/ConstructionLM.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: '#601821'"
     ]
    }
   ],
   "source": [
    "def create_bi_grams(path:str,vocabulary:dict)->dict:\n",
    "    \"\"\" \n",
    "    Create a dictionary with the bigrams and their counts\n",
    "    Params:\n",
    "    -------\n",
    "        path: path to the file of sentences\n",
    "    Returns:\n",
    "    --------\n",
    "        bi_grams: dictionary of dictionaries with the bigrams and their probabilities\n",
    "    \"\"\"\n",
    "    bi_grams = {word:{} for word in vocabulary}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    with open(path, \"r\") as f:\n",
    "        sentence = f.readline()\n",
    "        while len(sentence) != 0:\n",
    "            words = tokenizer.tokenize(sentence)\n",
    "            for i in range(len(words)-1):\n",
    "                if words[i+1] not in bi_grams[words[i]]:\n",
    "                    bi_grams[words[i]][words[i+1]] = 2\n",
    "                else:\n",
    "                    bi_grams[words[i]][words[i+1]] += 1\n",
    "            sentence = f.readline()\n",
    "\n",
    "    for bi_gram in bi_grams:\n",
    "        for word in bi_grams[bi_gram]:\n",
    "            bi_grams[bi_gram][word] /= (vocabulary[bi_gram] + vocab_size)\n",
    "    return bi_grams\n",
    "\n",
    "# news_bigram = create_bi_grams(PATH_NEWS_TRAIN, news_vocabulary)\n",
    "bac_bigram = create_bi_grams(PATH_BAC_TRAIN, bac_vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
