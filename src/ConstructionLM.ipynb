{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construccion del modelo de lenguaje\n",
    "## IMPORTANTE: Para ejecutar los notebooks, en la carpeta data a√±ada los archivos que puede encontrar en el siguiete drive:\n",
    "https://uniandes-my.sharepoint.com/:f:/g/personal/j_arboleda_uniandes_edu_co/EgEasT6fqDxFmBCiYZYRw0MBG87E7s4hFZuHCzTJ3DAXow?e=ybBMgw\n",
    "## Integrantes\n",
    "* Juan Esteban Arboleda\n",
    "* Luccas Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NEWS Dataset\n",
    "Lo primero que se llevara a cabo es la union de todos los documentos en un solo un par de archivos, uno con los documentos de 20news y otro con lso documentos de BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import time\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continucion se recorren todos los documentos de ambas carpetas y se unen en un solo archivo.\n",
    "* Se debe modificar la ruta de los archivos para que se ajuste. La ruta de la carpeta de 20news debe estar en PATH_20NEWS y la ruta de la carpeta de BAC debe estar en PATH_BAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_20NEWS = '../data/20news-18828'\n",
    "PATH_FINAL_20NEWS = \"../data/final_20news.txt\"\n",
    "\n",
    "def load_news(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "\n",
    "    \"\"\"\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\") as final_document:\n",
    "        for folder in os.listdir(documents_path):\n",
    "            for document_file in os.listdir(os.path.join(documents_path, folder)):\n",
    "                with open(os.path.join(documents_path, folder, document_file), \"r\") as document:\n",
    "                    text = document.read()\n",
    "                final_document.write(text)\n",
    "                final_document.write(\"\\n\")\n",
    "\n",
    "# load_news() should only be called if the PATH_FINAL_NEWS does not\n",
    "# exist yet.\n",
    "if not os.path.isfile(PATH_FINAL_20NEWS):\n",
    "    load_news(PATH_20NEWS,PATH_FINAL_20NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PATH_BAC= '../data/BAC/blogs/blogs'\n",
    "PATH_FINAL_BAC = \"../data/final_bac.txt\"\n",
    "\n",
    "def load_bac(documents_path: str,final_document_path:str):\n",
    "    \"\"\"\n",
    "    Creates a single file with all the documents in the folders\n",
    "    Params:\n",
    "    -------\n",
    "        documents_path: path to the folder with the documents\n",
    "        final_document_path: path to the final document\n",
    "    \"\"\"\n",
    "    pattern = r'<post>(.*?)</post>'\n",
    "    documents = []\n",
    "    index = []\n",
    "    id = 1\n",
    "    columns = ['filename', 'body']\n",
    "    open(final_document_path, \"w\").close()\n",
    "    with open(final_document_path, \"a\", encoding = 'latin_1') as final_document:\n",
    "        for file_name in os.listdir(documents_path):\n",
    "            with open(os.path.join(PATH_BAC,file_name) , encoding=\"latin_1\") as f:\n",
    "                text = f.read()\n",
    "                texts = re.findall(pattern, text, re.DOTALL)\n",
    "            all_text= \". \\n\".join(texts)\n",
    "            filtered_text = all_text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "            final_document.write(filtered_text)\n",
    "\n",
    "# load_bac() should only be called if the PATH_FINAL_BAC does not\n",
    "# exist yet.\n",
    "if not os.path.isfile(PATH_FINAL_BAC):\n",
    "    load_bac(PATH_BAC,PATH_FINAL_BAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_FINAL_20NEWS, \"r\", errors='ignore') as f:\n",
    "    raw_news = f.read()\n",
    "with open(PATH_FINAL_BAC, \"r\", errors='ignore') as f:\n",
    "    raw_bac = f.read()\n",
    "\n",
    "news_sentences = sent_tokenize(raw_news)\n",
    "# bac_sentences = sent_tokenize(raw_bac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente se leen los archivos y se normalizan con el formato adecuado para el modelo de lenguaje. De este modo se pone todo el minusculas, se remplazan los numeros por num y se agregan caracteres al inicio y al final de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence:str)->str:\n",
    "    \"\"\"\n",
    "    Normalize a sentence by lowercasing it, replacing numbers with NUM and adding <s> and </s> tokens\n",
    "    Params:\n",
    "    -------\n",
    "        sentence: sentence to normalize\n",
    "    Returns:\n",
    "    --------\n",
    "        sentence: normalized sentence\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = re.sub(r\"[^(a-zA-Z0-9\\s)]\", \" \", sentence).lower().replace(\"\\n\", \" \")\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    for word in words:\n",
    "        try:\n",
    "            word\n",
    "            number = float(word)\n",
    "            sentence = sentence.replace(word, \"NUM\")\n",
    "        except:\n",
    "            pass\n",
    "    sentence = f\"<s> {sentence} </s>\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego se extrae el vocabulario de todas las palabras junto con su frecuencia para asi poder reemplazar los tokens que no se encuentren en el vocabulario por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(sentences:list)->dict:\n",
    "    \"\"\"\n",
    "    Extract the vocabulary from a list of sentences\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "    Returns:\n",
    "    --------\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    for sentence in sentences:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = 1\n",
    "            else:   \n",
    "                vocabulary[word] += 1\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con el vocabulario y las frases se cambian todos los elementos que aparecen sola una vez en el corpus por el token \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknowns(sentences:list, vocabulary:dict)->list:\n",
    "    \"\"\"\n",
    "    Replace the words that appear only once in the corpus by the <UNK> token\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        vocabulary: dictionary in which the keys are the words and the values are the number of times the word appears in the corpus\n",
    "    Returns:\n",
    "    --------\n",
    "        sentences: list of sentences with the <UNK> token\n",
    "    \"\"\"\n",
    "    vocabulary[\"<UNK>\"] = 0\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for j, word in enumerate(words):\n",
    "            if vocabulary[word] == 1:\n",
    "                sentences[i] = sentence.replace(word, \"<UNK>\")\n",
    "                vocabulary[\"<UNK>\"] += 1\n",
    "                del vocabulary[word]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este punto se normalizan los 2 sets y se extrae su vocabulario para si poder remplazar los tokens que se encuentren en el vocabulario solo una vez por el token UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> from  mathew  mathew mantis co uk  subject  alt atheism faq  atheist resources  archive name  atheism resources alt atheism archive name  resources last modified  NUM december NUM version  NUM NUM                                atheist resources                        addresses of atheist organizations                                       usa  freedom from religion foundation  darwin fish bumper stickers and assorted other atheist paraphernalia are available from the freedom from religion foundation in the us  </s>\n"
     ]
    }
   ],
   "source": [
    "normalized_news_sentences = [normalize(sentence) for sentence in news_sentences]\n",
    "# normalized_bac_sentences = [normalize(sentence) for sentence in bac_sentences]\n",
    "print(normalized_news_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vocabulary = extract_vocabulary(normalized_news_sentences)\n",
    "# bac_vocabulary = extract_vocabulary(normalized_bac_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sentences = replace_unknowns(normalized_news_sentences, news_vocabulary)\n",
    "# bac_sentences = replace_unknowns(normalized_bac_sentences, bac_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "news_train, news_test = train_test_split(news_sentences, test_size=0.20)\n",
    "# bac_train, bac_test = train_test_split(bac_sentences, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_vocabulary = extract_vocabulary(news_train)\n",
    "# bac_vocabulary = extract_vocabulary(bac_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego guardamos estos datos en 4 archivos, 2 para train y 2 para test, uno para 20news y otro para BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_TRAIN = \"../data/20N_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_NEWS_TEST = \"../data/20N_l.rojasb_j.arboleda_test.txt\"  \n",
    "PATH_BAC_TRAIN = \"../data/BAC_l.rojasb_j.arboleda_training.txt\"\n",
    "PATH_BAC_TEST = \"../data/BAC_l.rojasb_j.arboleda_test.txt\" \n",
    "\n",
    "def save_file(sentences:list, path:str):\n",
    "    \"\"\"\n",
    "    Save a list of sentences in a file\n",
    "    Params:\n",
    "    -------\n",
    "        sentences: list of sentences\n",
    "        path: path to the file\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "save_file(news_train, PATH_NEWS_TRAIN)\n",
    "save_file(news_test, PATH_NEWS_TEST)\n",
    "# save_file(bac_train, PATH_BAC_TRAIN)\n",
    "# save_file(bac_test, PATH_BAC_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego pasamos a crear los n-gramas, para esto construimos diccionarios para cada combinacion posible de n-gramas, en este caso se construyen diccionario para los monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_UNIGRAM=\"../data/20N_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "PATH_BAC_UNIGRAM=\"../data/BAC_l.rojasb_j.arboleda_unigrams.txt\"\n",
    "\n",
    "def create_uni_grams(path: str, save_path: str)->dict:\n",
    "    \"\"\"\n",
    "    Creates and saves the monograms\n",
    "    \"\"\"\n",
    "    mono_grams = {}\n",
    "    total_words = 0\n",
    "    f = open(path, \"r\")\n",
    "    sf = open(save_path, \"w\")\n",
    "\n",
    "    sentence = f.readline()\n",
    "    while len(sentence) != 0:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for word in words:\n",
    "            total_words += 1\n",
    "            if word not in mono_grams:\n",
    "                mono_grams[word] = 1\n",
    "            else:\n",
    "                mono_grams[word] += 1\n",
    "        sentence = f.readline()\n",
    "            \n",
    "    for word in mono_grams:\n",
    "        prob = mono_grams[word] / total_words\n",
    "        sf.write(f\"{word},{mono_grams[word]},{prob}\\n\")\n",
    "\n",
    "create_uni_grams(PATH_NEWS_TRAIN, PATH_NEWS_UNIGRAM)\n",
    "# create_uni_grams(PATH_BAC_TRAIN, PATH_BAC_UNIGRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Posteriormente podemos guardar los diccionarios del unigrama en archivos para poder cargarlos posterioremtne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aca creamos los diccionarios para los bigramas y los guardamos en archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NEWS_BIGRAM=\"../data/20N_l.rojasb_j.arboleda_bigrams.txt\"\n",
    "PATH_BAC_BIGRAM=\"../data/BAC_l.rojasb_j.arboleda_bigrams.txt\"\n",
    "\n",
    "def create_bi_grams(sentence_path:str, vocabulary:dict, save_path: str)->dict:\n",
    "    \"\"\" \n",
    "    This function creates and saves a bigram. Returns the bigram count\n",
    "    \"\"\"\n",
    "    bigram_count = {}\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    f = open(sentence_path, \"r\")\n",
    "    sf = open(save_path, \"w\")\n",
    "    \n",
    "    sentence = f.readline()\n",
    "    while len(sentence) != 0:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for i in range(len(words)-1):\n",
    "            key = words[i] + \" \" + words[i+1]\n",
    "            if key not in bigram_count:\n",
    "                bigram_count[key] = 1\n",
    "            else:\n",
    "                bigram_count[key] += 1\n",
    "        sentence = f.readline()\n",
    "\n",
    "    for bigram in bigram_count:\n",
    "        words = bigram.split(\" \")\n",
    "        key = words[0] + \" \" + words[1]\n",
    "        prob = (bigram_count[key]+1)/(vocabulary[words[0]] + vocab_size)\n",
    "        sf.write(f\"{bigram},{bigram_count[key]},{prob}\\n\")\n",
    "            \n",
    "    return bigram_count\n",
    "\n",
    "news_bigrams_count = create_bi_grams(PATH_NEWS_TRAIN, news_vocabulary, PATH_NEWS_BIGRAM)\n",
    "# bac_bigrams_count = create_bi_grams(PATH_BAC_TRAIN, bac_vocabulary, PATH_BAC_BIGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "PATH_NEWS_TRIGRAM=\"../data/20N_l.rojasb_j.arboleda_trigrams.txt\"\n",
    "PATH_BAC_TRIGRAM=\"../data/BAC_l.rojasb_j.arboleda_trigrams.txt\"\n",
    "\n",
    "def create_tri_grams(sentence_path: str, vocabulary: dict, bi_grams_count: dict, save_path: str)->dict:\n",
    "    \"\"\" \n",
    "    Creates and saves the trigrams\n",
    "    \"\"\"\n",
    "    trigrams_count={}\n",
    "\n",
    "    bigram_size = len(bi_grams_count)\n",
    "    \n",
    "\n",
    "    f = open(sentence_path, \"r\")\n",
    "    sf = open(save_path, \"w\")\n",
    "\n",
    "    sentence = f.readline()\n",
    "    while len(sentence) != 0:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        for i in range(len(words)-2):\n",
    "            key = words[i] + \" \" + words[i+1] + \" \" + words[i+2]\n",
    "            if key not in trigrams_count:\n",
    "                trigrams_count[key] = 1\n",
    "            else:\n",
    "                trigrams_count[key] += 1\n",
    "        sentence = f.readline()\n",
    "\n",
    "    for tri_gram in trigrams_count:\n",
    "        words = tri_gram.split(\" \")\n",
    "        key = words[0] + \" \" + words[1] + \" \" + words[2]\n",
    "        prob = (trigrams_count[key] + 1) / (bi_grams_count[words[0] + \" \" + words[1]] + bi_grams_count)\n",
    "        sf.write(f\"{tri_gram},{trigrams_count[key]},{prob}\\n\")\n",
    "\n",
    "create_tri_grams(PATH_NEWS_TRAIN, news_vocabulary, news_bigrams_count, PATH_NEWS_TRIGRAM)\n",
    "print(\"done\")\n",
    "# create_tri_grams(PATH_BAC_TRAIN, bac_vocabulary, bac_bigrams_count, PATH_BAC_TRIGRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BAC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "bac_sentences = sent_tokenize(raw_bac)\n",
    "print(1)\n",
    "normalized_bac_sentences = [normalize(sentence) for sentence in bac_sentences]\n",
    "print(2)\n",
    "bac_vocabulary = extract_vocabulary(normalized_bac_sentences)\n",
    "print(3)\n",
    "bac_sentences = replace_unknowns(normalized_bac_sentences, bac_vocabulary)\n",
    "print(4)\n",
    "bac_train, bac_test = train_test_split(bac_sentences, test_size=0.20)\n",
    "print(5)\n",
    "bac_vocabulary = extract_vocabulary(bac_train)\n",
    "print(6)\n",
    "save_file(bac_train, PATH_BAC_TRAIN)\n",
    "save_file(bac_test, PATH_BAC_TEST)\n",
    "print(7)\n",
    "create_uni_grams(PATH_BAC_TRAIN, PATH_BAC_UNIGRAM)\n",
    "print(8)\n",
    "bac_bigrams_count = create_bi_grams(PATH_BAC_TRAIN, bac_vocabulary, PATH_BAC_BIGRAM)\n",
    "print(8)\n",
    "create_tri_grams(PATH_BAC_TRAIN, bac_vocabulary, bac_bigrams_count, PATH_BAC_TRIGRAM)\n",
    "print(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
